{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m _, frame \u001b[39m=\u001b[39m webcam\u001b[39m.\u001b[39mread()\n\u001b[0;32m     16\u001b[0m \u001b[39m# We send this frame to GazeTracking to analyze it\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m gaze\u001b[39m.\u001b[39;49mrefresh(frame)\n\u001b[0;32m     19\u001b[0m frame \u001b[39m=\u001b[39m gaze\u001b[39m.\u001b[39mannotated_frame()\n\u001b[0;32m     20\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Working\\Google\\Document\\Teaching-Project\\Python\\Affective Computing Course\\Combined\\gaze_tracking\\gaze_tracking\\gaze_tracking.py:63\u001b[0m, in \u001b[0;36mGazeTracking.refresh\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m\"\"\"Refreshes the frame and analyzes it.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[39mArguments:\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m    frame (numpy.ndarray): The frame to analyze\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframe \u001b[39m=\u001b[39m frame\n\u001b[1;32m---> 63\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_analyze()\n",
      "File \u001b[1;32mc:\\Working\\Google\\Document\\Teaching-Project\\Python\\Affective Computing Course\\Combined\\gaze_tracking\\gaze_tracking\\gaze_tracking.py:49\u001b[0m, in \u001b[0;36mGazeTracking._analyze\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     landmarks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predictor(frame, faces[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meye_left \u001b[39m=\u001b[39m Eye(frame, landmarks, \u001b[39m0\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalibration)\n\u001b[0;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meye_right \u001b[39m=\u001b[39m Eye(frame, landmarks, \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalibration)\n\u001b[0;32m     52\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Working\\Google\\Document\\Teaching-Project\\Python\\Affective Computing Course\\Combined\\gaze_tracking\\gaze_tracking\\eye.py:23\u001b[0m, in \u001b[0;36mEye.__init__\u001b[1;34m(self, original_frame, landmarks, side, calibration)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpupil \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlandmark_points \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_analyze(original_frame, landmarks, side, calibration)\n",
      "File \u001b[1;32mc:\\Working\\Google\\Document\\Teaching-Project\\Python\\Affective Computing Course\\Combined\\gaze_tracking\\gaze_tracking\\eye.py:113\u001b[0m, in \u001b[0;36mEye._analyze\u001b[1;34m(self, original_frame, landmarks, side, calibration)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblinking \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blinking_ratio(landmarks, points)\n\u001b[1;32m--> 113\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_isolate(original_frame, landmarks, points)\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m calibration\u001b[39m.\u001b[39mis_complete():\n\u001b[0;32m    116\u001b[0m     calibration\u001b[39m.\u001b[39mevaluate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframe, side)\n",
      "File \u001b[1;32mc:\\Working\\Google\\Document\\Teaching-Project\\Python\\Affective Computing Course\\Combined\\gaze_tracking\\gaze_tracking\\eye.py:51\u001b[0m, in \u001b[0;36mEye._isolate\u001b[1;34m(self, frame, landmarks, points)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m# Applying a mask to get only the eye\u001b[39;00m\n\u001b[0;32m     50\u001b[0m height, width \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m---> 51\u001b[0m black_frame \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros((height, width), np\u001b[39m.\u001b[39;49muint8)\n\u001b[0;32m     52\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((height, width), \u001b[39m255\u001b[39m, np\u001b[39m.\u001b[39muint8)\n\u001b[0;32m     53\u001b[0m cv2\u001b[39m.\u001b[39mfillPoly(mask, [region], (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demonstration of the GazeTracking library.\n",
    "Check the README.md for complete documentation.\n",
    "\"\"\"\n",
    "# Step 1: install initial libaries\n",
    "import cv2\n",
    "from gaze_tracking import GazeTracking\n",
    "\n",
    "gaze = GazeTracking()\n",
    "webcam = cv2.VideoCapture(0) # if you are using webcam change it to 1\n",
    "\n",
    "while True:\n",
    "    # We get a new frame from the webcam\n",
    "    _, frame = webcam.read()\n",
    "\n",
    "    # We send this frame to GazeTracking to analyze it\n",
    "    gaze.refresh(frame)\n",
    "\n",
    "    frame = gaze.annotated_frame()\n",
    "    text = \"\"\n",
    "\n",
    "    if gaze.is_blinking():\n",
    "        text = \"Blinking\"\n",
    "    elif gaze.is_right():\n",
    "        text = \"Looking right\"\n",
    "    elif gaze.is_left():\n",
    "        text = \"Looking left\"\n",
    "    elif gaze.is_center():\n",
    "        text = \"Looking center\"\n",
    "\n",
    "    cv2.putText(frame, text, (90, 60), cv2.FONT_HERSHEY_DUPLEX, 1.6, (147, 58, 31), 2) # it is for color and size of line on the eyes\n",
    "\n",
    "    left_pupil = gaze.pupil_left_coords()\n",
    "    right_pupil = gaze.pupil_right_coords()\n",
    "    cv2.putText(frame, \"Left pupil:  \" + str(left_pupil), (90, 130), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)\n",
    "    cv2.putText(frame, \"Right pupil: \" + str(right_pupil), (90, 165), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)\n",
    "\n",
    "    cv2.imshow(\"Demo\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "   \n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "affective_computing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
